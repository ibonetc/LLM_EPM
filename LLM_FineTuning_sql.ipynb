{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibonetc/LLM_EPM/blob/main/LLM_FineTuning_sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo de Fine Tuning para lenguaje SQL"
      ],
      "metadata": {
        "id": "ZMWZepP2kO3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c07a7a65-fccd-4add-9b96-c658fbf3e85f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JSwdRgVoiJ-1",
        "outputId": "1baea39f-13af-431f-fbcb-876caf4cff20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers peft accelerate bitsandbytes datasets -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "005bc652-2002-4f4d-88ed-ae526f365252",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "aV-yBLE1iJ-5"
      },
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6eeb55f8-1e2a-48ef-9251-01cebdbeaff5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sS7Nb-y3iJ-5"
      },
      "source": [
        "## Cargar datos de hugging face\n",
        "\n",
        "Base de datos **sql-create-context**, que tiene instrucciones para creación de diferentes códigos sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "174da008-85e4-4235-afd0-b45f2d7670f6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "abed5cb5ce024d88bb7e6a9f67c24d00"
          ]
        },
        "id": "bB_oEy_biJ-7",
        "outputId": "0684636c-e560-47bd-808d-fca30be6c089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abed5cb5ce024d88bb7e6a9f67c24d00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-07 01:37:39.544218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d9155a03-6a0d-44d0-bf84-33e0fdb1c4fd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "S3iAGCVEiJ-7"
      },
      "outputs": [],
      "source": [
        "token_file_path = '/Volumes/universidad_eia/default/isis/token_HF.txt'\n",
        "\n",
        "with open(token_file_path, 'r') as file:\n",
        "    token = file.read().strip()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d65579b1-1b98-4ab9-8564-704193307f22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "5e0f7739702141b78703f318153c370b",
            "c66b65bd96f7427385d12dce923ac4dd",
            "779b2697546042cf949c489048006104"
          ]
        },
        "id": "yMAfdX2xiJ-8",
        "outputId": "68f0f7b7-ab10-4dac-f12c-5717a1bf0698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:45: UserWarning: The cache_dir for this dataset is /root/.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS, UC Volumes].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/', '/Volumes/']\n  warnings.warn(warning_message)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e0f7739702141b78703f318153c370b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/4.43k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c66b65bd96f7427385d12dce923ac4dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sql_create_context_v4.json:   0%|          | 0.00/21.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "779b2697546042cf949c489048006104",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "38ad9934-ce38-4e31-a87c-82420d8f302f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZZ7T5Gd0iJ-8"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7c8a5da3-66ea-4acd-a346-6edf21a665af",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RRSKj89miJ-9",
        "outputId": "fa7965db-61aa-45a9-e8c3-d7ac12195d21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>\n",
              "      <td>How many heads of the departments are older than 56 ?</td>\n",
              "      <td>CREATE TABLE head (age INTEGER)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SELECT name, born_state, age FROM head ORDER BY age</td>\n",
              "      <td>List the name, born state and age of the heads of departments ordered by age.</td>\n",
              "      <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SELECT creation, name, budget_in_billions FROM department</td>\n",
              "      <td>List the creation year, name and budget of each department.</td>\n",
              "      <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department</td>\n",
              "      <td>What are the maximum and minimum budget of the departments?</td>\n",
              "      <td>CREATE TABLE department (budget_in_billions INTEGER)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15</td>\n",
              "      <td>What is the average number of employees of the departments whose rank is between 10 and 15?</td>\n",
              "      <td>CREATE TABLE department (num_employees INTEGER, ranking INTEGER)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                      answer  ...                                                                               context\n",
              "0                                   SELECT COUNT(*) FROM head WHERE age > 56  ...                                                       CREATE TABLE head (age INTEGER)\n",
              "1                        SELECT name, born_state, age FROM head ORDER BY age  ...                     CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)\n",
              "2                  SELECT creation, name, budget_in_billions FROM department  ...  CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\n",
              "3    SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department  ...                                  CREATE TABLE department (budget_in_billions INTEGER)\n",
              "4  SELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15  ...                      CREATE TABLE department (num_employees INTEGER, ranking INTEGER)\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {}
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "79bdd6a8-47c9-4a7a-a951-06061fe2c980",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CXRL9x-uiJ-9"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.max_rows\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dae7cafa-cc9c-4a74-b9de-2c3c8d44938b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WzRG-EmwiJ-9",
        "outputId": "9a50b0f6-550b-41dd-b60b-834c2136c460"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>\n",
              "      <td>How many heads of the departments are older than 56 ?</td>\n",
              "      <td>CREATE TABLE head (age INTEGER)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SELECT name, born_state, age FROM head ORDER BY age</td>\n",
              "      <td>List the name, born state and age of the heads of departments ordered by age.</td>\n",
              "      <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SELECT creation, name, budget_in_billions FROM department</td>\n",
              "      <td>List the creation year, name and budget of each department.</td>\n",
              "      <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department</td>\n",
              "      <td>What are the maximum and minimum budget of the departments?</td>\n",
              "      <td>CREATE TABLE department (budget_in_billions INTEGER)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15</td>\n",
              "      <td>What is the average number of employees of the departments whose rank is between 10 and 15?</td>\n",
              "      <td>CREATE TABLE department (num_employees INTEGER, ranking INTEGER)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                      answer  ...                                                                               context\n",
              "0                                   SELECT COUNT(*) FROM head WHERE age > 56  ...                                                       CREATE TABLE head (age INTEGER)\n",
              "1                        SELECT name, born_state, age FROM head ORDER BY age  ...                     CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)\n",
              "2                  SELECT creation, name, budget_in_billions FROM department  ...  CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\n",
              "3    SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department  ...                                  CREATE TABLE department (budget_in_billions INTEGER)\n",
              "4  SELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15  ...                      CREATE TABLE department (num_employees INTEGER, ranking INTEGER)\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3be08472-a9a3-48db-9f7a-33acc9723657",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "c_mCsDeJiJ--"
      },
      "source": [
        "## Dividir los datos en entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "43f1f688-8425-4824-889c-d3790df56ee4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RqmIpU4biJ--",
        "outputId": "1d2cd256-d8a0-45c5-b596-386a31d02da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de entrenamiento: 62861 pares de texto a SQL \nBase de prueba: 15716 pares texto a SQL \n"
          ]
        }
      ],
      "source": [
        "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"Base de entrenamiento: {len(train_dataset)} pares de texto a SQL \")\n",
        "print(f\"Base de prueba: {len(test_dataset)} pares texto a SQL \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4070176c-8707-4d3d-aa80-547d04a0f025",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-I8TgQZciJ-_"
      },
      "source": [
        "## Definir el prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d9534828-9a67-4561-a754-846f688d0776",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "o0RG6Rt_iJ-_"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n",
        "\n",
        "### Table:\n",
        "{context}\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83268bb4-13ab-4383-b517-5f0eef44727b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "b1ZtmNqQiJ-_"
      },
      "outputs": [],
      "source": [
        "def apply_prompt_template(row):\n",
        "    prompt = PROMPT_TEMPLATE.format(\n",
        "        question=row[\"question\"],\n",
        "        context=row[\"context\"],\n",
        "        output=row[\"answer\"],\n",
        "    )\n",
        "    return {\"prompt\": prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1e0405d2-75ee-4ce9-870d-546d32a15e0e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "4e0626292dc347b0b2806b7788d0668b"
          ]
        },
        "id": "69c0S-sHiJ-_",
        "outputId": "d474011c-cdaa-40df-f526-1b07f2ed7cb5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e0626292dc347b0b2806b7788d0668b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_dataset = train_dataset.map(apply_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d9fff92-2400-4afc-bda8-7358d22a6207",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RhzoyKHliJ_A"
      },
      "outputs": [],
      "source": [
        "df_train=train_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b2828324-86ae-4b8d-b138-e6ed3377dc39",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ePkbpEe2iJ_A",
        "outputId": "e74881aa-4081-4e19-91a0-0265bce3b893"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n",
              "      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td>\n",
              "      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td>\n",
              "      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\\n\\n### Table:\\nCREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)\\n\\n### Question:\\nWhich Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?\\n\\n### Response:\\nSELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                   answer  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          prompt\n",
              "0  SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"  ...  You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\\n\\n### Table:\\nCREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)\\n\\n### Question:\\nWhich Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?\\n\\n### Response:\\nSELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {}
        }
      ],
      "source": [
        "df_train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8e5ac8f6-9602-4ca9-8665-dcf2918ccd7c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j7_nfvJdiJ_A",
        "outputId": "831bba0f-6c8a-4abe-faa4-1fc9425a2e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\nCREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)\n\n### Question:\nWhich Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?\n\n### Response:\nSELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"\n"
          ]
        }
      ],
      "source": [
        "print(df_train['prompt'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4936fdd6-085f-4910-9f8a-f7ebec95f967",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "xZLxE3j-iJ_B"
      },
      "source": [
        "## Aplicar padding a la base de entrenamiento\n",
        "\n",
        "Asegurar que todas las secuencias de entrada en un batch son de la misma longitud\n",
        "\n",
        "* Padding a la derecha\n",
        "\n",
        "```\n",
        "Today |  is  |   a    |  cold  |  <pad>  ==generate=>  \"Today is a cold <pad> day\"\n",
        " How  |  to  | become |  <pad> |  <pad>  ==generate=>  \"How to become a <pad> <pad> great engineer\".\n",
        "```\n",
        "\n",
        "* Padding a la izquierda:\n",
        "\n",
        "```\n",
        "<pad> |  Today  |  is  |  a   |  cold     ==generate=>  \"<pad> Today is a cold day\"\n",
        "<pad> |  <pad>  |  How |  to  |  become   ==generate=>  \"<pad> <pad> How to become a great engineer\".\n",
        "```\n",
        "\n",
        "Normalmente se adicionan a la izquierda para que el modelo no tenga que adicionar tokens después de paddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ab9184c6-0365-4be1-9610-8aaf3f550ef7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "TFKhh3UXiJ_B"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "MAX_LENGTH = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "be3631a8-3ff0-4699-a34e-577dbc8bf797",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "322ecca886054d1d9a61472904d674d3",
            "1c32ba4644be4d08b5f08dd63d4becbc",
            "2886e1e02a4e40669bfa607f147e8a0c",
            "3469248d9ac84bf881b73ee297ffc552"
          ]
        },
        "id": "ccP0a9iQiJ_C",
        "outputId": "2bb70a6e-f63d-4ee1-8f3e-3c7b1b15167d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "322ecca886054d1d9a61472904d674d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c32ba4644be4d08b5f08dd63d4becbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2886e1e02a4e40669bfa607f147e8a0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3469248d9ac84bf881b73ee297ffc552",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=MAX_LENGTH,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    token=token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8b20f1cd-67df-41d0-9178-29b630485447",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "M2AqE8RxiJ_C"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7e1bb98d-b608-4560-97c2-7c84ba888015",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-vJVkomUiJ_D"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_pad_to_fixed_length(sample):\n",
        "    result = tokenizer(\n",
        "        sample[\"prompt\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f1e3d5d2-0e34-422d-a469-003b812867f5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "8941491a01ae488c99605ea4afee26d1"
          ]
        },
        "id": "wwoaV6JJiJ_D",
        "outputId": "128c68a1-149b-43d4-8e27-fa0f2e1a9d00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8941491a01ae488c99605ea4afee26d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize_and_pad_to_fixed_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "46adb772-d48a-412d-bfa1-3c433101363a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "48wWWcrQiJ_D"
      },
      "outputs": [],
      "source": [
        "assert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "abb31cdb-d39a-495d-97a1-cdf36f527173",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9am2x_oEiJ_D"
      },
      "source": [
        "## Cargar el modelo\n",
        "\n",
        "Utilizaremos el parámetro _quantization_config_. Este parámetro representa la técnica clave de **QLoRA** que reduce significativamente el uso de memoria durante el ajuste fino.\n",
        "_4-bit quantization_ reduce aún más el uso de memoria al aplicar una cuantificación de 4 bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "85c0746c-163f-434f-bc24-6c38a9ffd386",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "7ebd0722c0e84d5399eb24fe2ace2f1a",
            "46cad6b664a64e5ab322a5afd1f487ad",
            "8e18de944fa94a9588710d010ac8bc42",
            "3373b10d934048598f6ed82099c5e8ed",
            "0778431dc0cd48b4bef3d03cfb29b12e",
            "d6064491bea3481c8215390ad76d1633",
            "a7351245176145f381d6f47df66e37ea"
          ]
        },
        "id": "tT_I9EtUiJ_E",
        "outputId": "45c76aca-4649-4b0b-ed60-0969729f5d37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ebd0722c0e84d5399eb24fe2ace2f1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46cad6b664a64e5ab322a5afd1f487ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e18de944fa94a9588710d010ac8bc42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3373b10d934048598f6ed82099c5e8ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0778431dc0cd48b4bef3d03cfb29b12e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6064491bea3481c8215390ad76d1633",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7351245176145f381d6f47df66e37ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # 4-bit quantization\n",
        "    bnb_4bit_use_double_quant=True, # doble cuantificación\n",
        "    bnb_4bit_quant_type=\"nf4\", # Usar 4-bit para normal float\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Descuantificar los pesos a 16-bit después del backward\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config, token=token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a0ba482-5a77-46c3-8093-911ea64949bf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7-GdXy5UiJ_E",
        "outputId": "349663ec-9966-4b37-cfb6-1468d65ad051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "pipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
        "\n",
        "sample = test_dataset[1]\n",
        "prompt = PROMPT_TEMPLATE.format(\n",
        "    context=sample[\"context\"], question=sample[\"question\"], output=\"\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "64d63fb9-d2ff-4ff5-9f5b-c82f1425d62a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7ry7oAekiJ_E",
        "outputId": "a104a426-afc0-4b2e-e69f-f8c51605cb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt:\n You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\nCREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)\n\n### Question:\nWhat is the lowest numbered game against Phoenix with a record of 29-17?\n\n### Response:\n\ngenerated_query:\n SELECT * FROM table_name_61 WHERE game = 3 AND opponent = 'Phoenix' AND record = '29-17';\n\n### Explanation:\nThe answer is 3 because it is the lowest numbered game against Phoenix with a record of 29-17. The other games in this series were 4, 5, and 6.\n"
          ]
        }
      ],
      "source": [
        "print(\"prompt:\\n\", prompt)\n",
        "print(\"generated_query:\\n\", response[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4a22de28-2eb1-4bab-8495-f48f71346757",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PawhVHeEiJ_E"
      },
      "source": [
        "## Definir un modelo PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6c4ece94-da0c-46ec-b577-3bc7682e7096",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "SoCB2WcKiJ_F",
        "outputId": "65deb04c-5a25-4d65-a099-6ec3a5904bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1607\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=32, # para descomponer las matrices\n",
        "    lora_alpha=64, # coeficiente de aprendizaje ΔW\n",
        "    lora_dropout=0.1,\n",
        "    # fine-tune todas las capas lineales.\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b968afd-2b1d-4adc-944a-efd51ac71e7d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "06JpXUQviJ_F"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0cf96504-1617-4f6d-8d54-b8c5536e0c4f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HWAbXT7oiJ_F"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    # Set this to mlflow for logging your training\n",
        "    report_to=\"mlflow\",\n",
        "    # Name the MLflow run\n",
        "    run_name=f\"Mistral-7B-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n",
        "    # Replace with your output destination\n",
        "    output_dir=\"YOUR_OUTPUT_DIR\",\n",
        "\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    bf16=True,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    max_steps=500,\n",
        "    save_steps=100,\n",
        "    logging_steps=100,\n",
        "    warmup_steps=5,\n",
        "    ddp_find_unused_parameters=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "74b6bdff-86dd-49a8-8421-bd8e7db71ec9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3zC_QQwTiJ_G",
        "outputId": "824089ff-19d4-43a5-dd55-7624b11b5447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-07 02:21:39,856] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "df: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    args=training_args,\n",
        ")\n",
        "peft_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "31f56f2e-9e3b-4f33-8b1d-dd473a2d5637",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZrsaLIagiJ_G"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "560c81c4-8e6e-48d4-95eb-4a2e53a62889",
          "showTitle": false,
          "title": ""
        },
        "id": "mNm4LEccL9nk",
        "outputId": "ad7df89f-f35d-47ac-b797-94acef9e2175"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 45:41, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.522400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.507300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.494800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.474600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.5361956100463867, metrics={'train_runtime': 2747.9223, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.182, 'total_flos': 4.421038813216768e+16, 'train_loss': 0.5361956100463867, 'epoch': 0.06})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "926981c9-786d-4797-9605-e083720c7b0f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Q_tOkxxbiJ_H"
      },
      "source": [
        "## Parámetros de inferencia\n",
        "\n",
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6ed01bb6-1161-4198-825e-326eb6a583d7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GyiKLFEEiJ_H"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ab43358d-a1f3-4132-8ad7-56a390c91089",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "EcwQbYJiiJ_I"
      },
      "source": [
        "### Otros parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e8c345f8-c3fb-45ce-ae9f-0620aaf9c66b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rO-XpjHiiJ_I",
        "outputId": "c356135d-0523-4e17-e2c3-e1d0a03020ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "com.databricks.backend.common.rpc.CommandCancelledException\n",
              "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:445)\n",
              "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)\n",
              "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
              "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
              "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
              "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
              "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
              "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
              "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)\n",
              "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)\n",
              "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:820)\n",
              "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:846)\n",
              "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
              "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
              "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
              "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
              "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
              "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
              "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
              "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
              "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
              "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:845)\n",
              "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:900)\n",
              "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:693)\n",
              "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
              "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
              "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
              "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
              "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
              "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
              "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
              "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
              "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
              "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
              "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
              "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
              "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
              "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
              "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
              "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
              "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
              "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
              "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
              "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
              "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
              "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
              "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
              "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
              "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
              "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
              "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
              "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
              "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
              "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
              "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
              "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
              "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
              "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
              "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
              "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
              "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
              "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
              "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
              "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
              "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
              "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
              "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
              "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
              "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
              "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
              "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
              "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
              "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
              "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
              "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
              "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
              "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
              "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
              "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
              "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
              "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
              "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
              "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
              "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
              "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
              "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
              "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
              "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
              "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
              "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
              "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
              "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
              "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
              "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
              "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
              "\tat java.lang.Thread.run(Thread.java:750)"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "addedWidgets": {},
              "arguments": {},
              "datasetInfos": [],
              "jupyterProps": null,
              "metadata": {
                "errorSummary": "Cancelled"
              },
              "removedWidgets": [],
              "sqlProps": null,
              "stackFrames": [
                "com.databricks.backend.common.rpc.CommandCancelledException",
                "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:445)",
                "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)",
                "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
                "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
                "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
                "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
                "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
                "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
                "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)",
                "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
                "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)",
                "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)",
                "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:820)",
                "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:846)",
                "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
                "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
                "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
                "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
                "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
                "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
                "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
                "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
                "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
                "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:845)",
                "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:900)",
                "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:693)",
                "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
                "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
                "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
                "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
                "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
                "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
                "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
                "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
                "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
                "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
                "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
                "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
                "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
                "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
                "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
                "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
                "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
                "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
                "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
                "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
                "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
                "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
                "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
                "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
                "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
                "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
                "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
                "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
                "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
                "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
                "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
                "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
                "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
                "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
                "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
                "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
                "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
                "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
                "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
                "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
                "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
                "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
                "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
                "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
                "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
                "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
                "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
                "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
                "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
                "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
                "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
                "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
                "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
                "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
                "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
                "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
                "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
                "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
                "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
                "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
                "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
                "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
                "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
                "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
                "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
                "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
                "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
                "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
                "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
                "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
                "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
                "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
                "\tat java.lang.Thread.run(Thread.java:750)"
              ],
              "type": "baseError"
            }
          }
        }
      ],
      "source": [
        "from mlflow.models import infer_signature\n",
        "\n",
        "sample = train_dataset[1]\n",
        "\n",
        "# MLflow infers schema from the provided sample input/output/params\n",
        "signature = infer_signature(\n",
        "    model_input=sample[\"prompt\"],\n",
        "    model_output=sample[\"answer\"],\n",
        "    # Parameters are saved with default values if specified\n",
        "    params={\"max_new_tokens\": 256, \"repetition_penalty\": 1.15, \"return_full_text\": False},\n",
        ")\n",
        "signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3b0224c5-b156-4e13-970d-42980176f5f7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sgdw5MoLiJ_J"
      },
      "source": [
        "### Guardar el modelo PEFT a MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c8b57a3f-5c1a-4c65-b84b-ba4ea0b723c7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "wMdVnAkDiJ_J"
      },
      "outputs": [],
      "source": [
        "last_run_id = mlflow.last_active_run().info.run_id\n",
        "\n",
        "## Guardarlo sin padding\n",
        "tokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n",
        "\n",
        "with mlflow.start_run(run_id=last_run_id):\n",
        "  mlflow.log_params(peft_config.to_dict())\n",
        "  mlflow.transformers.log_model(\n",
        "    transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
        "    prompt_template=prompt_template,\n",
        "    signature=signature,\n",
        "    artifact_path=\"model\",  # poner una dirección para guardar el modelo\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fe4c2f10-b0dc-4104-8a19-28030b1617a4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "XLX9NtyXiJ_J"
      },
      "source": [
        "## Cargar el modelo para volver a usarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "93fef574-a782-4dd0-85f8-fec08196af5c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hZVVb-PPiJ_J"
      },
      "outputs": [],
      "source": [
        "#mlflow_model = mlflow.pyfunc.load_model(\"runs:/YOUR_RUN_ID/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c77c0a6c-f1ff-4ff4-aba1-334b78c53773",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-TcX1TM2iJ_K"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"\n",
        "### Table:\n",
        "CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)\n",
        "\n",
        "### Question:\n",
        "When Essendon played away; where did they play?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdpFA8bwL9no",
        "outputId": "68731331-abd6-4846-d78e-0025a1bcf63a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>generated_query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><br>### Table:<br>CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)<br><br>### Question:<br>When Essendon played away; where did they play?<br></td>\n",
              "      <td>SELECT venue FROM table_name_50 WHERE away_team = \"essendon\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generated_query = mlflow_model.predict(test_prompt)[0]\n",
        "df_result = pd.DataFrame({\"prompt\": test_prompt, \"generated_query\": generated_query})\n",
        "df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "aa24d0ae-4289-43f5-8d69-d0a09cb63dc0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "p4Gv3ZGfiJ_K"
      },
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "client": "1"
      },
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "LLM_FineTuning_sql",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}